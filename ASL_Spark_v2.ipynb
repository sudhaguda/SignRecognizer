{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import findspark\n",
    "# # findspark.init()\n",
    "# import cv2\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.image import ImageSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark # only run after findspark.init()\n",
    "# from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext, SparkConf\n",
    "# #from pyspark.sql import SparkSession\n",
    "# # conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "# # sc = pyspark.SparkContext(conf=conf)\n",
    "# spark = SparkSession.builder.config('spark.jars.packages','databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11')\n",
    "# # import sparkdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = (SparkSession\n",
    "#             .builder\n",
    "#             .config('spark.jars.packages', 'databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11')\n",
    "#             .config(\"spark.driver.memory\", \"8g\")\n",
    "#             .getOrCreate()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pyspark --conf spark.driver.memory=50g --conf spark.executor.pyspark.memory=50g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SUBMIT_ARGS = \"--packages databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.window import Window\n",
    "spark = SparkSession.builder.appName(\"DL with Spark Deep Cognition\").config(\"spark.driver.memory\", \"8g\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlcontext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkdl import DeepImageFeaturizer\n",
    "from sparkdl.image import imageIO\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from sparkdl import DeepImageFeaturizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import libraries\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Creating SparkSession\n",
    "# spark = (SparkSession\n",
    "#             .builder\n",
    "#             .config('spark.jars.packages', 'databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11')\n",
    "#             .getOrCreate()\n",
    "# )\n",
    "\n",
    "# # Import Spar-Deep-Learning-Pipelines\n",
    "# import sparkdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sparkdl import DeepImageFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dir = 'C:/Users/doudavi/Desktop/ignore/Sudha/Big_Data/Project/asl-alphabet/asl_alphabet_train/asl_alphabet_train'\n",
    "# test_dir = 'C:/Users/doudavi/Desktop/ignore/Sudha/Big_Data/Project/asl-alphabet/asl_alphabet_test/asl_alphabet_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = spark.read.format(\"image\").load(\"C:/Users/doudavi/Desktop/ignore/Sudha/Big_Data/Project/asl-alphabet/asl_alphabet_train/asl_alphabet_train/A/A1.jpg\")\n",
    "# # test1 = ImageSchema.readImages(\"C:/Users/Sudha/Desktop/IntoToBigDataAssignments/Projects/Data/asl-alphabet/asl_alphabet_train/asl_alphabet_train/A/A1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import lit\n",
    "# from sparkdl.image import imageIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# tulips_df = ImageSchema.readImages(\"flower_photos/tulips\").withColumn(\"label\", lit(1))\n",
    "# daisy_df = imageIO.readImagesWithCustomFn(\"flower_photos/daisy\", decode_f=imageIO.PIL_decode).withColumn(\"label\", lit(0))\n",
    "# tulips_train, tulips_test, _ = tulips_df.randomSplit([0.1, 0.05, 0.85])  # use larger training sets (e.g. [0.6, 0.4] for getting more images)\n",
    "# daisy_train, daisy_test, _ = daisy_df.randomSplit([0.1, 0.05, 0.85])     # use larger training sets (e.g. [0.6, 0.4] for getting more images)\n",
    "# train_df = tulips_train.unionAll(daisy_train)\n",
    "# test_df = tulips_test.unionAll(daisy_test)\n",
    "\n",
    "# # Under the hood, each of the partitions is fully loaded in memory, which may be expensive.\n",
    "# # This ensure that each of the paritions has a small size.\n",
    "# train_df = train_df.repartition(100)\n",
    "# test_df = test_df.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_df = ImageSchema.readImages('C:/Users/doudavi/Desktop/ignore/Sudha/Big_Data/Project/asl-alphabet/asl_alphabet_train/asl_alphabet_train/A/').withColumn(\"label\", lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daisy_df = imageIO.readImagesWithCustomFn(\"C://Users//doudavi//Desktop//ignore//Sudha//Big_Data//Project//asl-alphabet//asl_alphabet_train//asl_alphabet_train//A//\", decode_f=imageIO.PIL_decode).withColumn(\"label\", lit(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = imageIO.readImagesWithCustomFn(\"C:/Users/doudavi/Desktop/ignore/Sudha/Big_Data/Project/asl-alphabet/asl_alphabet_train/asl_alphabet_train/A/\", decode_f=imageIO.PIL_decode).withColumn(\"label\", lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = spark.read.format(\"image\").load(\"C:/Users/doudavi/Desktop/ignore/Sudha/Big_Data/Project/asl-alphabet/asl_alphabet_train/asl_alphabet_train/\").withColumn(\"label\", lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data (path):\n",
    "    labels_dict = {}\n",
    "    import os\n",
    "    val = 1\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in dirs:\n",
    "            labels_dict[name] = val\n",
    "            print(\"Reading ASL sign {} data\".format(name))\n",
    "            if val == 1:\n",
    "                spark_df = spark.read.format(\"image\").load(os.path.join(root, name)).withColumn(\"label\", lit(val))\n",
    "            else:\n",
    "                df_temp = spark.read.format(\"image\").load(os.path.join(root, name)).withColumn(\"label\", lit(val))\n",
    "                spark_df = spark_df.unionAll(df_temp)\n",
    "            val += 1\n",
    "    return spark_df, labels_dict\n",
    "#         print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ASL sign A data\n",
      "Reading ASL sign B data\n",
      "Reading ASL sign C data\n",
      "Reading ASL sign D data\n",
      "Reading ASL sign del data\n",
      "Reading ASL sign E data\n",
      "Reading ASL sign F data\n",
      "Reading ASL sign G data\n",
      "Reading ASL sign H data\n",
      "Reading ASL sign I data\n",
      "Reading ASL sign J data\n",
      "Reading ASL sign K data\n",
      "Reading ASL sign L data\n",
      "Reading ASL sign M data\n",
      "Reading ASL sign N data\n",
      "Reading ASL sign nothing data\n",
      "Reading ASL sign O data\n",
      "Reading ASL sign P data\n",
      "Reading ASL sign Q data\n",
      "Reading ASL sign R data\n",
      "Reading ASL sign S data\n",
      "Reading ASL sign space data\n",
      "Reading ASL sign T data\n",
      "Reading ASL sign U data\n",
      "Reading ASL sign V data\n",
      "Reading ASL sign W data\n",
      "Reading ASL sign X data\n",
      "Reading ASL sign Y data\n",
      "Reading ASL sign Z data\n"
     ]
    }
   ],
   "source": [
    "data, data_labels = read_data(\"C:/Users/doudavi/Desktop/ignore/Sudha/Big_Data/Project/asl-alphabet/asl_alphabet_train/asl_alphabet_train/\")\n",
    "# test_data, test_labels = read_data(\"C:/Users/doudavi/Desktop/ignore/Sudha/Big_Data/Project/asl-alphabet/asl_alphabet_train/asl_alphabet_train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = {}\n",
    "test_df = spark.read.format(\"image\").load(\"C:/Users/doudavi/Desktop/ignore/Sudha/Big_Data/Project/asl-alphabet/asl_alphabet_test/asl_alphabet_test\")\n",
    "for root, dirs, files in os.walk(\"C:/Users/doudavi/Desktop/ignore/Sudha/Big_Data/Project/asl-alphabet/asl_alphabet_test/asl_alphabet_test\"):\n",
    "    for file in files:\n",
    "#         print(os.path.join(root, file))\n",
    "        sign_val = file.split(\"_test.jpg\")[0]\n",
    "        if not sign_val in data_labels:\n",
    "            test_labels[sign_val] = max(data_labels, key=data_labels.get)+1\n",
    "        else:\n",
    "            test_labels[sign_val] = data_labels[sign_val]\n",
    "temp_df = sqlcontext.createDataFrame([(l,) for l in test_labels.values()], ['label'])\n",
    "test_df = test_df.withColumn(\"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "temp_df = temp_df.withColumn(\"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "test_df = test_df.join(temp_df, test_df.row_idx == temp_df.row_idx).drop(\"row_idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_dict = {}\n",
    "for i in range(1:30):\n",
    "    frac_dict[i] = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.sampleBy('label', fractions= frac_dict, seed=101)  # use larger training sets (e.g. [0.6, 0.4] for getting more images)\n",
    "val_data = data.substract(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image', 'label']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sampleBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sign data record count 87000\n",
      "training data record count 69546\n",
      "validation data record count 17454\n",
      "test data record count 28\n"
     ]
    }
   ],
   "source": [
    "print(\"sign data record count\", data.count())\n",
    "print(\"training data record count\", train_data.count())\n",
    "print(\"validation data record count\", val_data.count())\n",
    "print(\"test data record count\", test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sign data column count 2\n",
      "training data column count 2\n",
      "validation data column count 2\n",
      "test data column count 2\n"
     ]
    }
   ],
   "source": [
    "print(\"sign data column count\", len(data.columns))\n",
    "print(\"training data column count\", len(train_data.columns))\n",
    "print(\"validation data column count\", len(val_data.columns))\n",
    "print(\"test data column count\", len(test_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.repartition(100)\n",
    "val_data = val_data.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# # create generator\n",
    "# datagen = ImageDataGenerator()\n",
    "# # prepare an iterators for each dataset\n",
    "# train_it = datagen.flow_from_directory('C:/Users/doudavi/Desktop/ignore/Sudha/Big_Data/Project/asl-alphabet/asl_alphabet_train/asl_alphabet_train/', class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = train_it.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_images_A = spark.read.format(\"image\").load(\"C:/Users/doudavi/Desktop/ignore/Sudha/Big_Data/Project/asl-alphabet/asl_alphabet_train/asl_alphabet_train/A/\").withColumn(\"label\", lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_images_B = spark.read.format(\"image\").load(\"C:/Users/doudavi/Desktop/ignore/Sudha/Big_Data/Project/asl-alphabet/asl_alphabet_train/asl_alphabet_train/B/\").withColumn(\"label\", lit(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_images_A.show(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tulips_train, tulips_test, _ = df_images_A.randomSplit([0.1, 0.05, 0.85])  # use larger training sets (e.g. [0.6, 0.4] for getting more images)\n",
    "# daisy_train, daisy_test, _ = df_images_B.randomSplit([0.1, 0.05, 0.85])     # use larger training sets (e.g. [0.6, 0.4] for getting more images)\n",
    "# train_df = tulips_train.unionAll(daisy_train)\n",
    "# test_df = tulips_test.unionAll(daisy_test)\n",
    "\n",
    "# # Under the hood, each of the partitions is fully loaded in memory, which may be expensive.\n",
    "# # This ensure that each of the paritions has a small size.\n",
    "# train_df = train_df.repartition(100)\n",
    "# test_df = test_df.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o311.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 15.0 failed 1 times, most recent failure: Lost task 7.0 in stage 15.0 (TID 16385, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 135266 ms\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1137)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)\r\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\r\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\r\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-7603b8559507>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeaturizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mp_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# must be an Estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \"\"\"\n\u001b[0;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o311.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 15.0 failed 1 times, most recent failure: Lost task 7.0 in stage 15.0 (TID 16385, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 135266 ms\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1137)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)\r\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\r\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\r\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "featurizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\", modelName=\"InceptionV3\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.05, elasticNetParam=0.3, labelCol=\"label\")\n",
    "p = Pipeline(stages=[featurizer, lr])\n",
    "\n",
    "p_model = p.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "tested_df = p_model.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(test_df.select(\"prediction\", \"label\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import InceptionV3\n",
    "\n",
    "model = InceptionV3(weights=\"imagenet\")\n",
    "model.save('model-full.h5')  # saves to the local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StringType\n",
    "from sparkdl import KerasImageFileTransformer\n",
    "\n",
    "def loadAndPreprocessKerasInceptionV3(uri):\n",
    "  # this is a typical way to load and prep images in keras\n",
    "    image = img_to_array(load_img(uri, target_size=(299, 299)))  # image dimensions for InceptionV3\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    return preprocess_input(image)\n",
    "\n",
    "transformer = KerasImageFileTransformer(inputCol=\"uri\", outputCol=\"predictions\",\n",
    "                                        modelFile='model-full.h5',  # local file path for model\n",
    "                                        imageLoader=loadAndPreprocessKerasInceptionV3,\n",
    "                                        outputMode=\"vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkdl import KerasTransformer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Generate random input data\n",
    "num_features = 10\n",
    "num_examples = 100\n",
    "input_data = [{\"features\" : np.random.randn(num_features).astype(float).tolist()} for i in range(num_examples)]\n",
    "schema = StructType([ StructField(\"features\", ArrayType(FloatType()), True)])\n",
    "input_df = spark.createDataFrame(input_data, schema)\n",
    "\n",
    "# Create and save a single-hidden-layer Keras model for binary classification\n",
    "# NOTE: In a typical workflow, we'd train the model before exporting it to disk,\n",
    "# but we skip that step here for brevity\n",
    "model = Sequential()\n",
    "model.add(Dense(units=20, input_shape=[num_features], activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model_path = \"simple-binary-classification\"\n",
    "model.save(model_path)\n",
    "\n",
    "# Create transformer and apply it to our input data\n",
    "transformer = KerasTransformer(inputCol=\"features\", outputCol=\"predictions\", modelFile=model_path)\n",
    "final_df = transformer.transform(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "    \n",
    "model.add(Conv2D(64, kernel_size=4, strides=1, activation='relu', input_shape=TARGET_DIMS))\n",
    "model.add(Conv2D(64, kernel_size=4, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(128, kernel_size=4, strides=1, activation='relu'))\n",
    "model.add(Conv2D(128, kernel_size=4, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(256, kernel_size=4, strides=1, activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=4, strides=2, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_df = imageIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['image']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.select([\"image\", \"data\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import namedtuple\n",
    "# #  NOTE: likely to be migrated to Spark ImageSchema code in the near future.\n",
    "# _OcvType = namedtuple(\"OcvType\", [\"name\", \"ord\", \"nChannels\", \"dtype\"])\n",
    "\n",
    "# _SUPPORTED_OCV_TYPES = (\n",
    "#     _OcvType(name=\"CV_8UC1\", ord=0, nChannels=1, dtype=\"uint8\"),\n",
    "#     _OcvType(name=\"CV_32FC1\", ord=5, nChannels=1, dtype=\"float32\"),\n",
    "#     _OcvType(name=\"CV_8UC3\", ord=16, nChannels=3, dtype=\"uint8\"),\n",
    "#     _OcvType(name=\"CV_32FC3\", ord=21, nChannels=3, dtype=\"float32\"),\n",
    "#     _OcvType(name=\"CV_8UC4\", ord=24, nChannels=4, dtype=\"uint8\"),\n",
    "#     _OcvType(name=\"CV_32FC4\", ord=29, nChannels=4, dtype=\"float32\"),\n",
    "# )\n",
    "\n",
    "# #  NOTE: likely to be migrated to Spark ImageSchema code in the near future.\n",
    "# _OCV_TYPES_BY_NAME = {m.name: m for m in _SUPPORTED_OCV_TYPES}\n",
    "# _OCV_TYPES_BY_ORDINAL = {m.ord: m for m in _SUPPORTED_OCV_TYPES}\n",
    "\n",
    "\n",
    "# def imageTypeByOrdinal(ordinal):\n",
    "#     if not ordinal in _OCV_TYPES_BY_ORDINAL:\n",
    "#         raise KeyError(\"unsupported image type with ordinal %d, supported OpenCV types = %s\" % (\n",
    "#             ordinal, str(_SUPPORTED_OCV_TYPES)))\n",
    "#     return _OCV_TYPES_BY_ORDINAL[ordinal]\n",
    "\n",
    "# def imageStructToArray(imageRow):\n",
    "#     \"\"\"\n",
    "#     Convert an image to a numpy array.\n",
    "#     :param imageRow: Row, must use imageSchema.\n",
    "#     :return: ndarray, image data.\n",
    "#     \"\"\"\n",
    "#     imType = imageTypeByOrdinal(imageRow.mode)\n",
    "#     shape = (imageRow.height, imageRow.width, imageRow.nChannels)\n",
    "#     return np.ndarray(shape, imType.dtype, imageRow.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import org.apache.spark.ml.image.ImageSchema._\n",
    "# import java.nio.file.Paths\n",
    "\n",
    "# val images = readImages(\"path/to/images\")\n",
    "\n",
    "# images.foreach { rrow =>\n",
    "#   val row = rrow.getAs[Row](0)\n",
    "#   val filename = Paths.get(getOrigin(row)).getFileName().toString()\n",
    "#   val imageData = getData(row)\n",
    "#   val height = getHeight(row)\n",
    "#   val width = getWidth(row)\n",
    "\n",
    "#   println(s\"${height}x${width}\")\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size_img = 64,64 \n",
    "# images_for_plot = []\n",
    "# labels_for_plot = []\n",
    "# for folder in os.listdir(train_dir):\n",
    "#     for file in os.listdir(train_dir + '/' + folder):\n",
    "#         filepath = train_dir + '/' + folder + '/' + file\n",
    "#         image = cv2.imread(filepath)\n",
    "# #         print(\"Image print\")\n",
    "# #         print(image)\n",
    "#         final_img = cv2.resize(image, size_img)\n",
    "# #         print(\"final Image print\")\n",
    "# #         print(final_img)\n",
    "# #         ary = imageStructToArray(final_img)\n",
    "# #         final_img = Image.fromarray(obj=ary, mode='RGB')\n",
    "#         imageio.core.util.Image img = imageio.imread(final_img, as_gray=False, pilmode=\"RGB\")\n",
    "#         #final_img = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)\n",
    "#         #image_df = spark.read.format(\"image\").load(final_img)\n",
    "# #       print(final_img)\n",
    "#         #print(type(imref[0][0]))\n",
    "#         images_for_plot.append(img)\n",
    "#         labels_for_plot.append(folder)\n",
    "#         break\n",
    "# # return images_for_plot, labels_for_plot\n",
    "# print(images_for_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size_img = 64,64 \n",
    "# images_for_plot = []\n",
    "# labels_for_plot = []\n",
    "# for folder in os.listdir(train_dir):\n",
    "#     for file in os.listdir(train_dir + '/' + folder):\n",
    "#         filepath = train_dir + '/' + folder + '/' + file\n",
    "#         image = cv2.imread(filepath)\n",
    "#         print(\"Image print\")\n",
    "#         print(image)\n",
    "#         final_img = cv2.resize(image, size_img)\n",
    "#         print(\"final Image print\")\n",
    "#         print(final_img)\n",
    "#         ary = imageStructToArray(final_img)\n",
    "#         final_img = Image.fromarray(obj=ary, mode='RGB')\n",
    "# #         imageio.core.util.Image img = imageio.imread(final_img, as_gray=False, pilmode=\"RGB\")\n",
    "#         final_img = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)\n",
    "#         image_df = spark.read.format(\"image\").load(final_img)\n",
    "# #       print(final_img)\n",
    "#         print(type(imref[0][0]))\n",
    "#         images_for_plot.append(img)\n",
    "#         labels_for_plot.append(folder)\n",
    "#         break\n",
    "# # return images_for_plot, labels_for_plot\n",
    "# print(images_for_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_images():\n",
    "#     size_img = 64,64 \n",
    "#     images_for_plot = []\n",
    "#     labels_for_plot = []\n",
    "#     for folder in os.listdir(dir_path):\n",
    "#         for file in os.listdir(dir_path + '/' + folder):\n",
    "#             filepath = dir_path + '/' + folder + '/' + file\n",
    "#             image = cv2.imread(filepath)\n",
    "#             final_img = cv2.resize(image, size_img)\n",
    "#             final_img = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)\n",
    "#             images_for_plot.append(final_img)\n",
    "#             labels_for_plot.append(folder)\n",
    "#             break\n",
    "#     return images_for_plot, labels_for_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import StringType\n",
    "# from pyspark.sql.functions import udf, col\n",
    "# def say_hello(name : str) -> str:\n",
    "#      return f\"Hello {name}\"\n",
    "# assert say_hello(\"Summer\") == \"Hello Summer\"\n",
    "# say_hello_udf = udf(lambda name: say_hello(name), StringType())\n",
    "# df = spark.createDataFrame([(\"Rick\",),(\"Morty\",)], [\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_udf = udf(my_udf, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.withColumn(\"greetings\", say_hello_udf(col(\"name\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = cv2.imread('C:/Users/doudavi/Desktop/ignore/Sudha/Big_Data/Project/asl-alphabet/asl_alphabet_train/asl_alphabet_train/A/A1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_img = cv2.resize(image, size_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_img[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_img1 = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(final_img1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.sql('''select 'spark' as hello ''')\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np # We'll be storing our data as numpy arrays\n",
    "# import os # For handling directories\n",
    "# from PIL import Image # For handling the images\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda list|grep \"sparkdl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
